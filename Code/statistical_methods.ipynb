{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cc6e3db",
   "metadata": {},
   "source": [
    "## Import & global declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39b7129d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from itertools import islice\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "\n",
    "# Local imports\n",
    "from filters import *\n",
    "\n",
    "# Global declarations\n",
    "ALGORITHM = DSOR\n",
    "INPUT_FOLDER = 'input'\n",
    "OUTPUT_FOLDER = ALGORITHM.__name__\n",
    "DATASET_FOLDERS = ['11','12','13','14','15','16','17','18','20','22','23','24','26','28','30','34','35','36','76']\n",
    "MAX_LEN = 1 # max amount of frames to read from each sequence (set to none to run full set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a542e8e",
   "metadata": {},
   "source": [
    "## Local declarations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16448323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input function\n",
    "def read_kitty(dir_names:list, return_labels:bool=True, max_len:int=None, input_folder:str='input'):\n",
    "    \"\"\"\n",
    "    Opens a point cloud dataset following kitty formatting\n",
    "    \n",
    "    Parameters:\n",
    "    dir_name: list of names of directorys containing a kitty sequence\n",
    "    lables: if true, reads and returns labels as well\n",
    "    max_len: maximum amount of point clouds to read from each directory\n",
    "    input_folder: name of head folder containing the datasets\n",
    "    \n",
    "    Returns: Dictionaries for names, scans and optionally labels. Each with directory\n",
    "    name as key and a list as value\n",
    "    \"\"\"\n",
    "\n",
    "    names = {n:[] for n in dir_names}\n",
    "    scans = {n:[] for n in dir_names}\n",
    "    labels = {n:[] for n in dir_names}\n",
    "    for dir_name in dir_names:\n",
    "        # get datapaths\n",
    "        scan_path = os.path.join(input_folder, dir_name, 'velodyne')\n",
    "        scan_names = sorted([os.path.join(dp, f) \n",
    "                      for dp, dn, fn in os.walk(os.path.expanduser(scan_path)) \n",
    "                      for f in fn])\n",
    "\n",
    "        label_path = os.path.join(input_folder, dir_name, 'labels')\n",
    "        label_names = sorted([os.path.join(dp, f) \n",
    "                   for dp, dn, fn in os.walk(os.path.expanduser(label_path)) \n",
    "                   for f in fn])\n",
    "\n",
    "        # assert all files have corresponding labels\n",
    "        scan_path_len = len(scan_path)+1\n",
    "        label_path_len = len(label_path)+1\n",
    "        file_names = [scan[scan_path_len:-4] for scan in scan_names]\n",
    "        assert all(name == label[label_path_len:-6] for name, label in zip(file_names, label_names)), f'file name missmatch in dir {dir_name}'\n",
    "        \n",
    "        names[dir_name] = file_names if not max_len else file_names[:max_len]\n",
    "\n",
    "        # iterate datapaths\n",
    "        iterator = zip(scan_names, label_names)\n",
    "        iterator = iterator if not max_len else islice(iterator, max_len)\n",
    "        for bin_file_name, label_file_name in iterator:\n",
    "\n",
    "            # open and read pointcloud\n",
    "            with open(bin_file_name, mode='rb') as file:\n",
    "                scan = np.fromfile(file, dtype=np.float32)\n",
    "                scan = np.reshape(scan, (-1,4))\n",
    "                scans[dir_name] += [scan]\n",
    "\n",
    "            # same for labels\n",
    "            if return_labels:  \n",
    "                with open(label_file_name, mode='rb') as file:\n",
    "                    label = np.fromfile(file, dtype=np.int16)\n",
    "                    label = np.reshape(label, (-1,2))\n",
    "                    labels[dir_name] += [label]\n",
    "                assert label.shape[0] == scan.shape[0] # assert 1 to 1 ratio\n",
    "\n",
    "    if return_labels:\n",
    "        return names, scans, labels \n",
    "    return names, scans\n",
    "\n",
    "def write_one_kitty(dir_name:str, dataframes, labels=None, names=None):\n",
    "    \"writes numpy point cloud data as kitty format\"\n",
    "    output_path = os.path.join(dir_name)\n",
    "    names = names if names else [str(i) for i in range(len(dataframes))]\n",
    "    if not os.path.isdir(output_path):\n",
    "        os.mkdir(output_path)\n",
    "        os.mkdir(os.path.join(dir_name, 'velodyne'))\n",
    "        os.mkdir(os.path.join(dir_name, 'labels'))\n",
    "        \n",
    "    # make velodyne data\n",
    "    vel_dir = os.path.join(dir_name, 'velodyne')\n",
    "    for data, name in zip(dataframes, names):\n",
    "        data.tofile(f'{vel_dir}/{name}.bin', sep='', format='%s')\n",
    "    \n",
    "    # make label data\n",
    "    if labels:\n",
    "        lab_dir = os.path.join(dir_name, 'labels')\n",
    "        for data, name in zip(labels, names):\n",
    "            data.tofile(f'{lab_dir}/{name}.label', sep='', format='%s') \n",
    "\n",
    "# output function\n",
    "def write_kitty(scans:dict, labels:dict=None, names:dict=None, folder:str='output'):\n",
    "    \"\"\"\n",
    "    Writes a point cloud dataset following kitty formatting\n",
    "    \n",
    "    Parameters:\n",
    "    scans: dictonary with directory name as key and a list of point clouds as value\n",
    "    labels: optional dictonary with directory name as key and a list of labels as value\n",
    "    names: dictonary with directory name as key and a list of file names as value\n",
    "    folder: name of the top folder to put all output files and folders into\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(folder):\n",
    "        os.mkdir(os.path.join(folder))\n",
    "    keys = [*scans.keys()]\n",
    "    for key in keys:\n",
    "        kwargs = {\n",
    "            'labels':None, \n",
    "            'names':None}\n",
    "        if names:\n",
    "            kwargs.update({'names':names[key]})\n",
    "        if labels:\n",
    "            kwargs.update({'labels':labels[key]})\n",
    "        write_one_kitty(os.path.join(folder,key), scans[key], **kwargs)\n",
    "\n",
    "\n",
    "def tuner(X, y, func, metric, params:dict, max_iter:int=10):\n",
    "    \"\"\"\n",
    "    X: train data\n",
    "    y: data labels\n",
    "    func: algoritm function to call\n",
    "    metric: evaluation metric to optimise\n",
    "    param: dict with function params as key and as value: tuple of \n",
    "        (start value, increment function, decrement function)\n",
    "    max_iter: loop will break after this many laps or if no update happens. \n",
    "        Can be set to None for exhaustive search (use with care)\n",
    "    \"\"\"\n",
    "    metrics = {'metric':metric}\n",
    "    met = lambda p: evaluate(X, y, func, metrics, target_label=110, **p)['metric'].mean()\n",
    "    \n",
    "    best = {key:val for key,(val,_,_) in params.items()}\n",
    "    best = (met(best), best) # baseline\n",
    "    tested = [best] # track tested params\n",
    "    count=0 # track loop count\n",
    "    \n",
    "    def equal_dict(a,b):\n",
    "        \"checks if dicts contain equal values\"\n",
    "        return sorted(a.items()) == sorted(b.items())\n",
    "        \n",
    "    while True:\n",
    "        old_best = best[1].copy()\n",
    "        \n",
    "        for param,(_,up,down) in params.items():\n",
    "            _, b = best # unpack best params\n",
    "            up_params, down_params = b.copy(), b.copy() # copy best\n",
    "            up_params.update( {param:up(b[param])} ) # replace one param\n",
    "            down_params.update( {param:down(b[param])} ) # replace one param\n",
    "            results = [(met(p), p) for p in [down_params,up_params]]\n",
    "            tested += results # add runs to log\n",
    "            results += [best] # add baseline to compare\n",
    "            new_best = max(results,key=lambda x:x[0]) # find new best\n",
    "            best = new_best if new_best[0] > best[0] else best # only update on strictly greater than to avoid deadlock\n",
    "            \n",
    "        if equal_dict(old_best,best[1]): # if no update\n",
    "            break\n",
    "            \n",
    "        count += 1\n",
    "        if max_iter:\n",
    "            if max_iter<=count:\n",
    "                break\n",
    "                \n",
    "    return best, tested\n",
    "\n",
    "def evaluate(points, labels, filter_func, metrics:dict, target_label:int=110, **func_kwargs): \n",
    "    import pandas as pd\n",
    "    from itertools import chain\n",
    "    \n",
    "    flat_points = chain.from_iterable(points.values())\n",
    "    flat_labels = chain.from_iterable(labels.values())\n",
    "    \n",
    "    mets=np.ndarray((0, len(metrics)+1) ,dtype=float)\n",
    "    for X, y in zip(flat_points, flat_labels):\n",
    "        outlier_labels = [target_label]\n",
    "        start = time.time()\n",
    "        mask = filter_func(X, **func_kwargs)\n",
    "        m = [time.time()-start]\n",
    "        m += [func(y, mask) for key, func in metrics.items()]\n",
    "        mets = np.concatenate([mets,[m]])\n",
    "    \n",
    "    ind = [[key]*len(points[key]) for key in points.keys()]\n",
    "    ind = chain.from_iterable(ind)\n",
    "    return pd.DataFrame(mets,columns=['Time',*metrics.keys()],index=ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37e9c07",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5690c3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup():\n",
    "    names, points, labels = read_kitty(DATASET_FOLDERS, max_len=None, input_folder=INPUT_FOLDER)\n",
    "\n",
    "    for folder, frames in points.items():\n",
    "        l = labels[folder]\n",
    "        assert len(l) == len(frames)\n",
    "        for set_id, (label, frame) in enumerate(zip(l, frames)):\n",
    "            P, index_map = np.unique(points[folder][set_id], return_index=True, axis=0)\n",
    "            points[folder][set_id] = P\n",
    "            labels[folder][set_id] = labels[folder][set_id][index_map]\n",
    "\n",
    "    write_kitty(points, labels=labels, names=names, folder='clean')\n",
    "    \n",
    "# cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb32aab9",
   "metadata": {},
   "source": [
    "## Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d58f7148",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Point cloud shape: (115881, 4)\n",
      "[-197.84776     26.662176     3.9865794   13.       ]\n",
      "[-197.78802     25.835665     2.8061757    6.       ]\n",
      "[-197.77983     25.658987     1.0338181   10.       ]\n",
      "[-197.77368     26.125235     2.2110257    9.       ]\n",
      "[-197.7581      26.23899      3.3880968   13.       ]\n",
      "\n",
      "Label shape: (115881, 2)\n",
      "[0 0]\n",
      "[0 0]\n",
      "[0 0]\n",
      "[0 0]\n",
      "[0 0]\n"
     ]
    }
   ],
   "source": [
    "names, points, labels = read_kitty(DATASET_FOLDERS, max_len=MAX_LEN, input_folder=INPUT_FOLDER)\n",
    "print('Point cloud shape:', points['11'][0].shape)\n",
    "print(*points['11'][0][:5],sep='\\n')\n",
    "print()\n",
    "print('Label shape:', labels['11'][0].shape)\n",
    "print(*labels['11'][0][:5],sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1295f1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_labels = [110] # 110:label for snow in the air => outliers\n",
    "binary_labels = {key:[np.isin(frame[:,0], outlier_labels) for frame in frames] for key, frames in labels.items()}\n",
    "#binary_labels = np.isin(labels['11'][0][:,0], outlier_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1cb8946",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write_kitty(points, labels, names, folder=OUTPUT_FOLDER) # No-op write to test function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e79fa7",
   "metadata": {},
   "source": [
    "## Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "408ce5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# og params on https://bitbucket.org/autonomymtu/dsor_filter/src/master/launch/example.launch\n",
    "\n",
    "increment_k = lambda k:k+2\n",
    "decrement_k = lambda k:max(k-1,0)\n",
    "increment_float = lambda f:f*2\n",
    "decrement_float = lambda f:f*2/3\n",
    "\n",
    "params = {\n",
    "    'Sfactor':(0.001,increment_float,decrement_float),\n",
    "    'r':(0.1,increment_float,decrement_float),\n",
    "    'k':(4,increment_k,decrement_k),\n",
    "}\n",
    "\n",
    "alg = iterate_filter(DSOR, 2)\n",
    "best, test_log = tuner(points, binary_labels, alg, f1_score, params, max_iter=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c931c2d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0.8447144253595866, {'Sfactor': 0.001, 'r': 0.1, 'k': 4}),\n",
       " [(0.8447144253595866, {'Sfactor': 0.001, 'r': 0.1, 'k': 4}),\n",
       "  (0.844655449277386, {'Sfactor': 0.0006666666666666666, 'r': 0.1, 'k': 4}),\n",
       "  (0.8444879139304178, {'Sfactor': 0.002, 'r': 0.1, 'k': 4}),\n",
       "  (0.798532613558412, {'Sfactor': 0.001, 'r': 0.06666666666666667, 'k': 4}),\n",
       "  (0.40927462242076157, {'Sfactor': 0.001, 'r': 0.2, 'k': 4}),\n",
       "  (0.8402296017184914, {'Sfactor': 0.001, 'r': 0.1, 'k': 3}),\n",
       "  (0.84038588792533, {'Sfactor': 0.001, 'r': 0.1, 'k': 6})])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best,test_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6ea9b5e0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "metrics = {\n",
    "    'precision':precision_score, \n",
    "    'recall':recall_score, \n",
    "    'accuracy':accuracy_score,\n",
    "    'f1':f1_score,\n",
    "}\n",
    "result = evaluate(points, binary_labels, alg, metrics, target_label=110, **best[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8ffb9cab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.529797</td>\n",
       "      <td>0.84201</td>\n",
       "      <td>0.847436</td>\n",
       "      <td>0.961616</td>\n",
       "      <td>0.844714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.529797</td>\n",
       "      <td>0.84201</td>\n",
       "      <td>0.847436</td>\n",
       "      <td>0.961616</td>\n",
       "      <td>0.844714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.529797</td>\n",
       "      <td>0.84201</td>\n",
       "      <td>0.847436</td>\n",
       "      <td>0.961616</td>\n",
       "      <td>0.844714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.529797</td>\n",
       "      <td>0.84201</td>\n",
       "      <td>0.847436</td>\n",
       "      <td>0.961616</td>\n",
       "      <td>0.844714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.529797</td>\n",
       "      <td>0.84201</td>\n",
       "      <td>0.847436</td>\n",
       "      <td>0.961616</td>\n",
       "      <td>0.844714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.529797</td>\n",
       "      <td>0.84201</td>\n",
       "      <td>0.847436</td>\n",
       "      <td>0.961616</td>\n",
       "      <td>0.844714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Time  precision    recall  accuracy        f1\n",
       "count  1.000000    1.00000  1.000000  1.000000  1.000000\n",
       "mean   0.529797    0.84201  0.847436  0.961616  0.844714\n",
       "std         NaN        NaN       NaN       NaN       NaN\n",
       "min    0.529797    0.84201  0.847436  0.961616  0.844714\n",
       "25%    0.529797    0.84201  0.847436  0.961616  0.844714\n",
       "50%    0.529797    0.84201  0.847436  0.961616  0.844714\n",
       "75%    0.529797    0.84201  0.847436  0.961616  0.844714\n",
       "max    0.529797    0.84201  0.847436  0.961616  0.844714"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65828409",
   "metadata": {},
   "source": [
    "## Anomaly detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9256fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = points['11'][0]\n",
    "metrics = {\n",
    "    'precision':precision_score, \n",
    "    'recall':recall_score, \n",
    "    'accuracy':accuracy_score\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "4d9c65ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09999913704576247"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_kernel = lambda f: 'poly'\n",
    "another_kernel = lambda f:'sigmoid'\n",
    "increment_percent = lambda f:(f+1.0)/2\n",
    "decrement_percent = lambda f:(f+0.0)/2\n",
    "\n",
    "params = {\n",
    "    'kernel':('rbf',a_kernel,another_kernel),\n",
    "    'contamination':(0.1,increment_percent,decrement_percent),\n",
    "}\n",
    "\n",
    "best, test_log = tuner(points, binary_labels, LOF, accuracy_score, params, max_iter=1)\n",
    "print(best)\n",
    "mask = OCS(frame[:,:3], contamination = 0.1)\n",
    "[(key, func(binary_labels, mask)) for key, func in metrics.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "23af06ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('precision', 0.3072143596824301),\n",
       " ('recall', 0.24936957130848977),\n",
       " ('accuracy', 0.8382478577161053)]"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "    'k':(5,increment_k,decrement_k),\n",
    "}\n",
    "\n",
    "best, test_log = tuner(points, binary_labels, LOF, accuracy_score, params, max_iter=3)\n",
    "print(best)\n",
    "mask = LOF(frame[:,:3], metric='l1',contamination=0.1, **best)\n",
    "[(key, func(binary_labels, mask)) for key, func in metrics.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "3545fe72",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = EE(frame[:,:3], contamination = 0.1)\n",
    "[(key, func(binary_labels, mask)) for key, func in metrics.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "58c3fd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "increment_int = lambda f:f*2\n",
    "decrement_int = lambda f:f*2//3\n",
    "increment_percent = lambda f:(f+1.0)/2\n",
    "decrement_percent = lambda f:(f+0.0)/2\n",
    "\n",
    "params = {\n",
    "    'n_estimators':(100,increment_int,decrement_int),\n",
    "    'max_features':(1.0,increment_percent,decrement_percent)\n",
    "}\n",
    "\n",
    "best, test_log = tuner(points, binary_labels, IF, accuracy_score, params, max_iter=3)\n",
    "print(best)\n",
    "mask = IF(frame[:,:3], contamination = 0.1, **best)\n",
    "[(key, func(binary_labels, mask)) for key, func in metrics.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4831e4a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
